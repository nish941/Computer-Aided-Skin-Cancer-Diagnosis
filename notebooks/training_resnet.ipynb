# training_resnet.ipynb - Complete Model Training Notebook

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, applications
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import (
    ModelCheckpoint, 
    EarlyStopping, 
    ReduceLROnPlateau, 
    TensorBoard
)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import os
from sklearn.utils import class_weight
from sklearn.metrics import classification_report, confusion_matrix

# Configuration
CONFIG = {
    'img_size': (224, 224),
    'batch_size': 32,
    'epochs': 50,
    'learning_rate': 1e-4,
    'num_classes': 7,
    'class_names': ['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC'],
    'data_dir': 'path/to/ISIC_2018_data'
}

def load_and_preprocess_data():
    """Load and preprocess ISIC 2018 dataset"""
    # Load metadata
    metadata = pd.read_csv(os.path.join(CONFIG['data_dir'], 'metadata.csv'))
    
    # Handle class imbalance
    class_counts = metadata['diagnosis'].value_counts()
    print("Class distribution:")
    print(class_counts)
    
    # Calculate class weights
    class_weights = compute_class_weights(metadata['diagnosis'])
    
    # Split data
    from sklearn.model_selection import train_test_split
    train_df, val_df = train_test_split(
        metadata, 
        test_size=0.2, 
        stratify=metadata['diagnosis'],
        random_state=42
    )
    
    return train_df, val_df, class_weights

def compute_class_weights(labels):
    """Compute class weights for imbalanced dataset"""
    class_weights = class_weight.compute_class_weight(
        'balanced',
        classes=np.unique(labels),
        y=labels
    )
    return dict(enumerate(class_weights))

def create_data_generators(train_df, val_df):
    """Create data generators with augmentation"""
    
    # Data augmentation for training
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        vertical_flip=True,
        brightness_range=[0.8, 1.2],
        zoom_range=[0.9, 1.1],
        fill_mode='nearest'
    )
    
    # Validation generator (no augmentation)
    val_datagen = ImageDataGenerator(rescale=1./255)
    
    # Create generators
    train_generator = train_datagen.flow_from_dataframe(
        dataframe=train_df,
        directory=CONFIG['data_dir'],
        x_col='image_path',
        y_col='diagnosis',
        target_size=CONFIG['img_size'],
        batch_size=CONFIG['batch_size'],
        class_mode='categorical',
        shuffle=True
    )
    
    val_generator = val_datagen.flow_from_dataframe(
        dataframe=val_df,
        directory=CONFIG['data_dir'],
        x_col='image_path',
        y_col='diagnosis',
        target_size=CONFIG['img_size'],
        batch_size=CONFIG['batch_size'],
        class_mode='categorical',
        shuffle=False
    )
    
    return train_generator, val_generator

def build_resnet50_model():
    """Build ResNet-50 model with custom head"""
    
    # Load pre-trained ResNet-50
    base_model = applications.ResNet50(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )
    
    # Freeze base model layers
    base_model.trainable = False
    
    # Add custom head
    inputs = keras.Input(shape=(224, 224, 3))
    
    # Preprocessing (same as ResNet)
    x = applications.resnet.preprocess_input(inputs)
    
    # Base model
    x = base_model(x, training=False)
    
    # Global pooling
    x = layers.GlobalAveragePooling2D()(x)
    
    # Dropout for regularization
    x = layers.Dropout(0.5)(x)
    
    # Dense layers
    x = layers.Dense(256, activation='relu')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.3)(x)
    
    # Output layer
    outputs = layers.Dense(CONFIG['num_classes'], activation='softmax')(x)
    
    # Create model
    model = keras.Model(inputs, outputs)
    
    return model, base_model

def compile_model(model):
    """Compile the model with optimizer and loss"""
    
    optimizer = keras.optimizers.Adam(
        learning_rate=CONFIG['learning_rate'],
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07
    )
    
    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=[
            'accuracy',
            keras.metrics.Precision(name='precision'),
            keras.metrics.Recall(name='recall'),
            keras.metrics.AUC(name='auc')
        ]
    )
    
    return model

def create_callbacks():
    """Create training callbacks"""
    
    # Model checkpoint
    checkpoint = ModelCheckpoint(
        'models/resnet50_best.h5',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    )
    
    # Early stopping
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1
    )
    
    # Learning rate scheduler
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )
    
    # TensorBoard
    tensorboard = TensorBoard(
        log_dir='logs',
        histogram_freq=1
    )
    
    return [checkpoint, early_stopping, reduce_lr, tensorboard]

def train_model(model, train_gen, val_gen, class_weights, callbacks):
    """Train the model"""
    
    history = model.fit(
        train_gen,
        epochs=CONFIG['epochs'],
        validation_data=val_gen,
        class_weight=class_weights,
        callbacks=callbacks,
        verbose=1
    )
    
    return history

def evaluate_model(model, test_gen):
    """Evaluate model on test set"""
    
    # Get predictions
    y_pred = model.predict(test_gen)
    y_pred_classes = np.argmax(y_pred, axis=1)
    
    # Get true labels
    y_true = test_gen.classes
    
    # Classification report
    print("Classification Report:")
    print(classification_report(
        y_true, 
        y_pred_classes, 
        target_names=CONFIG['class_names']
    ))
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred_classes)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=CONFIG['class_names'],
                yticklabels=CONFIG['class_names'])
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    plt.show()
    
    # Calculate metrics
    accuracy = np.trace(cm) / np.sum(cm)
    sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])  # Example for binary
    
    print(f"Overall Accuracy: {accuracy:.3f}")
    print(f"Sensitivity: {sensitivity:.3f}")
    
    return y_pred_classes, cm

def plot_training_history(history):
    """Plot training history"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Accuracy
    axes[0, 0].plot(history.history['accuracy'], label='Train')
    axes[0, 0].plot(history.history['val_accuracy'], label='Validation')
    axes[0, 0].set_title('Model Accuracy')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # Loss
    axes[0, 1].plot(history.history['loss'], label='Train')
    axes[0, 1].plot(history.history['val_loss'], label='Validation')
    axes[0, 1].set_title('Model Loss')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # Precision
    axes[1, 0].plot(history.history['precision'], label='Train')
    axes[1, 0].plot(history.history['val_precision'], label='Validation')
    axes[1, 0].set_title('Model Precision')
    axes[1, 0].set_ylabel('Precision')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # Recall
    axes[1, 1].plot(history.history['recall'], label='Train')
    axes[1, 1].plot(history.history['val_recall'], label='Validation')
    axes[1, 1].set_title('Model Recall')
    axes[1, 1].set_ylabel('Recall')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

def fine_tune_model(model, base_model, train_gen, val_gen, callbacks):
    """Fine-tune the model by unfreezing some layers"""
    
    # Unfreeze top layers of base model
    base_model.trainable = True
    
    # Freeze first 100 layers
    for layer in base_model.layers[:100]:
        layer.trainable = False
    
    # Recompile with lower learning rate
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-5),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # Continue training
    fine_tune_history = model.fit(
        train_gen,
        epochs=10,
        validation_data=val_gen,
        callbacks=callbacks,
        verbose=1
    )
    
    return fine_tune_history

def save_model(model, filename='models/resnet50_model.h5'):
    """Save the trained model"""
    model.save(filename)
    print(f"Model saved to {filename}")

def main():
    """Main training pipeline"""
    
    print("üöÄ Starting Skin Cancer Diagnosis Model Training")
    print("=" * 50)
    
    # Step 1: Load data
    print("\nüìä Step 1: Loading and preprocessing data...")
    train_df, val_df, class_weights = load_and_preprocess_data()
    
    # Step 2: Create data generators
    print("\nüîÑ Step 2: Creating data generators...")
    train_gen, val_gen = create_data_generators(train_df, val_df)
    
    # Step 3: Build model
    print("\nüèóÔ∏è Step 3: Building ResNet-50 model...")
    model, base_model = build_resnet50_model()
    model.summary()
    
    # Step 4: Compile model
    print("\n‚öôÔ∏è Step 4: Compiling model...")
    model = compile_model(model)
    
    # Step 5: Create callbacks
    print("\nüîî Step 5: Setting up callbacks...")
    callbacks = create_callbacks()
    
    # Step 6: Train model
    print("\nüéØ Step 6: Training model...")
    history = train_model(model, train_gen, val_gen, class_weights, callbacks)
    
    # Step 7: Fine-tune
    print("\nüîß Step 7: Fine-tuning model...")
    fine_tune_history = fine_tune_model(model, base_model, train_gen, val_gen, callbacks)
    
    # Step 8: Evaluate
    print("\nüìà Step 8: Evaluating model...")
    y_pred, cm = evaluate_model(model, val_gen)
    
    # Step 9: Plot history
    print("\nüìä Step 9: Plotting training history...")
    plot_training_history(history)
    
    # Step 10: Save model
    print("\nüíæ Step 10: Saving model...")
    save_model(model)
    
    print("\n‚úÖ Training completed successfully!")
    print(f"üìÅ Model saved to: models/resnet50_model.h5")

if __name__ == "__main__":
    main()
